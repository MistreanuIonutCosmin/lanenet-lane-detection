{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import os.path as ops\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import glog as log\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import global_config\n",
    "from lanenet_model import lanenet_merge_model\n",
    "from data_provider import lanenet_data_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = global_config.cfg\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale(input_arr):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_arr:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    min_val = np.min(input_arr)\n",
    "    max_val = np.max(input_arr)\n",
    "\n",
    "    output_arr = (input_arr - min_val) * 255.0 / (max_val - min_val)\n",
    "\n",
    "    return output_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(dataset_dir, weights_path=None, net_flag='vgg'):\n",
    "    \"\"\"\n",
    "\n",
    "    :param dataset_dir:\n",
    "    :param net_flag: choose which base network to use\n",
    "    :param weights_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    #path/train.txt\n",
    "    train_dataset_file = ops.join(dataset_dir, 'train.txt')\n",
    "    val_dataset_file = ops.join(dataset_dir, 'val.txt')\n",
    "\n",
    "    assert ops.exists(train_dataset_file)\n",
    "\n",
    "    train_dataset = lanenet_data_processor.DataSet(train_dataset_file)\n",
    "    val_dataset = lanenet_data_processor.DataSet(val_dataset_file)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        input_tensor = tf.placeholder(dtype=tf.float32,\n",
    "                                      shape=[CFG.TRAIN.BATCH_SIZE, CFG.TRAIN.IMG_HEIGHT,\n",
    "                                             CFG.TRAIN.IMG_WIDTH, 3],\n",
    "                                      name='input_tensor')\n",
    "        binary_label_tensor = tf.placeholder(dtype=tf.int64,\n",
    "                                             shape=[CFG.TRAIN.BATCH_SIZE, CFG.TRAIN.IMG_HEIGHT,\n",
    "                                                    CFG.TRAIN.IMG_WIDTH, 1],\n",
    "                                             name='binary_input_label')\n",
    "        instance_label_tensor = tf.placeholder(dtype=tf.float32,\n",
    "                                               shape=[CFG.TRAIN.BATCH_SIZE, CFG.TRAIN.IMG_HEIGHT,\n",
    "                                                      CFG.TRAIN.IMG_WIDTH],\n",
    "                                               name='instance_input_label')\n",
    "        phase = tf.placeholder(dtype=tf.string, shape=None, name='net_phase')\n",
    "\n",
    "        net = lanenet_merge_model.LaneNet(net_flag=net_flag, phase=phase)\n",
    "\n",
    "        # calculate the loss\n",
    "        compute_ret = net.compute_loss(input_tensor=input_tensor, binary_label=binary_label_tensor,\n",
    "                                       instance_label=instance_label_tensor, ignore_label=255, name='lanenet_model')\n",
    "        total_loss = compute_ret['total_loss']\n",
    "        binary_seg_loss = compute_ret['binary_seg_loss']\n",
    "        disc_loss = compute_ret['discriminative_loss']\n",
    "        pix_embedding = compute_ret['instance_seg_logits']\n",
    "\n",
    "        # calculate the accuracy\n",
    "        out_logits = compute_ret['binary_seg_logits']\n",
    "        out_logits = tf.nn.softmax(logits=out_logits)\n",
    "        out_logits_out = tf.argmax(out_logits, axis=-1)\n",
    "        out = tf.argmax(out_logits, axis=-1)\n",
    "        out = tf.expand_dims(out, axis=-1)\n",
    "\n",
    "        idx = tf.where(tf.equal(binary_label_tensor, 1))\n",
    "        pix_cls_ret = tf.gather_nd(out, idx)\n",
    "        accuracy = tf.count_nonzero(pix_cls_ret)\n",
    "        accuracy = tf.divide(accuracy, tf.cast(tf.shape(pix_cls_ret)[0], tf.int64))\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(CFG.TRAIN.LEARNING_RATE, global_step,\n",
    "                                                   100000, 0.1, staircase=True)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.MomentumOptimizer(\n",
    "                learning_rate=learning_rate, momentum=0.9).minimize(loss=total_loss,\n",
    "                                                                    var_list=tf.trainable_variables(),\n",
    "                                                                    global_step=global_step)\n",
    "\n",
    "    # Set tf saver\n",
    "    saver = tf.train.Saver()\n",
    "    model_save_dir = '/workspace/storage/projects/lanenet-lane-detection/test_ignore_label'\n",
    "    if not ops.exists(model_save_dir):\n",
    "        os.makedirs(model_save_dir)\n",
    "    train_start_time = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime(time.time()))\n",
    "    \n",
    "  \n",
    "    \n",
    "    model_name = 'tusimple_lanenet_{:s}_{:s}.ckpt'.format(net_flag, str(train_start_time))\n",
    "    model_save_path = ops.join(model_save_dir, model_name)\n",
    "\n",
    "    # Set tf summary\n",
    "    tboard_save_path = 'tboard/tusimple_lanenet/test_ignore_label/{:s}'.format(net_flag)\n",
    "    if not ops.exists(tboard_save_path):\n",
    "        os.makedirs(tboard_save_path)\n",
    "    train_cost_scalar = tf.summary.scalar(name='train_cost', tensor=total_loss)\n",
    "    val_cost_scalar = tf.summary.scalar(name='val_cost', tensor=total_loss)\n",
    "    train_accuracy_scalar = tf.summary.scalar(name='train_accuracy', tensor=accuracy)\n",
    "    val_accuracy_scalar = tf.summary.scalar(name='val_accuracy', tensor=accuracy)\n",
    "    train_binary_seg_loss_scalar = tf.summary.scalar(name='train_binary_seg_loss', tensor=binary_seg_loss)\n",
    "    val_binary_seg_loss_scalar = tf.summary.scalar(name='val_binary_seg_loss', tensor=binary_seg_loss)\n",
    "    train_instance_seg_loss_scalar = tf.summary.scalar(name='train_instance_seg_loss', tensor=disc_loss)\n",
    "    val_instance_seg_loss_scalar = tf.summary.scalar(name='val_instance_seg_loss', tensor=disc_loss)\n",
    "    learning_rate_scalar = tf.summary.scalar(name='learning_rate', tensor=learning_rate)\n",
    "    train_merge_summary_op = tf.summary.merge([train_accuracy_scalar, train_cost_scalar,\n",
    "                                               learning_rate_scalar, train_binary_seg_loss_scalar,\n",
    "                                               train_instance_seg_loss_scalar])\n",
    "    val_merge_summary_op = tf.summary.merge([val_accuracy_scalar, val_cost_scalar,\n",
    "                                             val_binary_seg_loss_scalar, val_instance_seg_loss_scalar])\n",
    "\n",
    "    # Set sess configuration\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TRAIN.GPU_MEMORY_FRACTION\n",
    "    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n",
    "    sess_config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "    sess = tf.Session(config=sess_config)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(tboard_save_path)\n",
    "    summary_writer.add_graph(sess.graph)\n",
    "\n",
    "    # Set the training parameters\n",
    "    train_epochs = CFG.TRAIN.EPOCHS\n",
    "\n",
    "    log.info('Global configuration is as follows:')\n",
    "    log.info(CFG)\n",
    "\n",
    "    with sess.as_default():\n",
    "\n",
    "        tf.train.write_graph(graph_or_graph_def=sess.graph, logdir='',\n",
    "                             name='{:s}/lanenet_model.pb'.format(model_save_dir))\n",
    "\n",
    "        if weights_path is None:\n",
    "            log.info('Training from scratch')\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "        else:\n",
    "            log.info('Restore model from last model checkpoint {:s}'.format(weights_path))\n",
    "            saver.restore(sess=sess, save_path=weights_path)\n",
    "\n",
    "        # Load pre-training parameters\n",
    "        if net_flag == 'vgg' and weights_path is None:\n",
    "            pretrained_weights = np.load(\n",
    "                './data/vgg16.npy',\n",
    "                encoding='latin1').item()\n",
    "\n",
    "            for vv in tf.trainable_variables():\n",
    "                weights_key = vv.name.split('/')[-3]\n",
    "                try:\n",
    "                    weights = pretrained_weights[weights_key][0]\n",
    "                    _op = tf.assign(vv, weights)\n",
    "                    sess.run(_op)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "        train_cost_time_mean = []\n",
    "        val_cost_time_mean = []\n",
    "        \n",
    "        ignore_labels = cv2.imread(\"/workspace/storage/datasets/AVMSnapshots/AVM/ignore_labels.png\")\n",
    "        for epoch in range(train_epochs):\n",
    "            # training part\n",
    "            t_start = time.time()\n",
    "\n",
    "            with tf.device('/cpu:0'):\n",
    "                gt_imgs, binary_gt_labels, instance_gt_labels = train_dataset.next_batch(CFG.TRAIN.BATCH_SIZE, ignore_label=ignore_labels)\n",
    "                \n",
    "                # resized the GTs in order to improve speed, use this resize if you don't have data images size 256x512\n",
    "                \n",
    "#                 gt_imgs = [cv2.resize(tmp,\n",
    "#                                       dsize=(CFG.TRAIN.IMG_WIDTH, CFG.TRAIN.IMG_HEIGHT),\n",
    "#                                       dst=tmp,\n",
    "#                                       interpolation=cv2.INTER_LINEAR)\n",
    "#                            for tmp in gt_imgs]\n",
    "\n",
    "                gt_imgs = [tmp - VGG_MEAN for tmp in gt_imgs]\n",
    "                \n",
    "#                 binary_gt_labels = [cv2.resize(tmp,\n",
    "#                                                dsize=(CFG.TRAIN.IMG_WIDTH, CFG.TRAIN.IMG_HEIGHT),\n",
    "#                                                dst=tmp,\n",
    "#                                                interpolation=cv2.INTER_NEAREST)\n",
    "#                                     for tmp in binary_gt_labels]\n",
    "\n",
    "                binary_gt_labels = [np.expand_dims(tmp, axis=-1) for tmp in binary_gt_labels]\n",
    "#                 instance_gt_labels = [cv2.resize(tmp,\n",
    "#                                                  dsize=(CFG.TRAIN.IMG_WIDTH, CFG.TRAIN.IMG_HEIGHT),\n",
    "#                                                  dst=tmp,\n",
    "#                                                  interpolation=cv2.INTER_NEAREST)\n",
    "#                                       for tmp in instance_gt_labels]\n",
    "            phase_train = 'train'\n",
    "\n",
    "            _, c, train_accuracy, train_summary, binary_loss, instance_loss, embedding, binary_seg_img = \\\n",
    "                sess.run([optimizer, total_loss,\n",
    "                          accuracy,\n",
    "                          train_merge_summary_op,\n",
    "                          binary_seg_loss,\n",
    "                          disc_loss,\n",
    "                          pix_embedding,\n",
    "                          out_logits_out],\n",
    "                         feed_dict={input_tensor: gt_imgs,\n",
    "                                    binary_label_tensor: binary_gt_labels,\n",
    "                                    instance_label_tensor: instance_gt_labels,\n",
    "                                    phase: phase_train})\n",
    "\n",
    "            if math.isnan(c) or math.isnan(binary_loss) or math.isnan(instance_loss):\n",
    "                log.error('cost is: {:.5f}'.format(c))\n",
    "                log.error('binary cost is: {:.5f}'.format(binary_loss))\n",
    "                log.error('instance cost is: {:.5f}'.format(instance_loss))\n",
    "                cv2.imwrite('nan_image.png', gt_imgs[0] + VGG_MEAN)\n",
    "                cv2.imwrite('nan_instance_label.png', instance_gt_labels[0])\n",
    "                cv2.imwrite('nan_binary_label.png', binary_gt_labels[0] * 255)\n",
    "                return\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                cv2.imwrite('image.png', gt_imgs[0] + VGG_MEAN)\n",
    "                cv2.imwrite('binary_label.png', binary_gt_labels[0] * 255)\n",
    "                cv2.imwrite('instance_label.png', instance_gt_labels[0])\n",
    "                cv2.imwrite('binary_seg_img.png', binary_seg_img[0] * 255)\n",
    "\n",
    "                for i in range(4):\n",
    "                    embedding[0][:, :, i] = minmax_scale(embedding[0][:, :, i])\n",
    "                embedding_image = np.array(embedding[0], np.uint8)\n",
    "                cv2.imwrite('embedding.png', embedding_image)\n",
    "\n",
    "            cost_time = time.time() - t_start\n",
    "            train_cost_time_mean.append(cost_time)\n",
    "            summary_writer.add_summary(summary=train_summary, global_step=epoch)\n",
    "\n",
    "            # validation part\n",
    "            with tf.device('/cpu:0'):\n",
    "                gt_imgs_val, binary_gt_labels_val, instance_gt_labels_val \\\n",
    "                    = val_dataset.next_batch(CFG.TRAIN.VAL_BATCH_SIZE, ignore_label=ignore_labels)\n",
    "#                 gt_imgs_val = [cv2.resize(tmp,\n",
    "#                                           dsize=(CFG.TRAIN.IMG_WIDTH, CFG.TRAIN.IMG_HEIGHT),\n",
    "#                                           dst=tmp,\n",
    "#                                           interpolation=cv2.INTER_LINEAR)\n",
    "#                                for tmp in gt_imgs_val]\n",
    "                gt_imgs_val = [tmp - VGG_MEAN for tmp in gt_imgs_val]\n",
    "#                 binary_gt_labels_val = [cv2.resize(tmp,\n",
    "#                                                    dsize=(CFG.TRAIN.IMG_WIDTH, CFG.TRAIN.IMG_HEIGHT),\n",
    "#                                                    dst=tmp)\n",
    "#                                         for tmp in binary_gt_labels_val]\n",
    "                binary_gt_labels_val = [np.expand_dims(tmp, axis=-1) for tmp in binary_gt_labels_val]\n",
    "#                 instance_gt_labels_val = [cv2.resize(tmp,\n",
    "#                                                      dsize=(CFG.TRAIN.IMG_WIDTH, CFG.TRAIN.IMG_HEIGHT),\n",
    "#                                                      dst=tmp,\n",
    "#                                                      interpolation=cv2.INTER_NEAREST)\n",
    "#                                           for tmp in instance_gt_labels_val]\n",
    "            phase_val = 'test'\n",
    "\n",
    "            t_start_val = time.time()\n",
    "            c_val, val_summary, val_accuracy, val_binary_seg_loss, val_instance_seg_loss = \\\n",
    "                sess.run([total_loss, val_merge_summary_op, accuracy, binary_seg_loss, disc_loss],\n",
    "                         feed_dict={input_tensor: gt_imgs_val,\n",
    "                                    binary_label_tensor: binary_gt_labels_val,\n",
    "                                    instance_label_tensor: instance_gt_labels_val,\n",
    "                                    phase: phase_val})\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                cv2.imwrite('test_image.png', gt_imgs_val[0] + VGG_MEAN)\n",
    "\n",
    "            summary_writer.add_summary(val_summary, global_step=epoch)\n",
    "\n",
    "            cost_time_val = time.time() - t_start_val\n",
    "            val_cost_time_mean.append(cost_time_val)\n",
    "\n",
    "            if epoch % CFG.TRAIN.DISPLAY_STEP == 0:\n",
    "                log.info('Epoch: {:d} total_loss= {:6f} binary_seg_loss= {:6f} instance_seg_loss= {:6f} accuracy= {:6f}'\n",
    "                         ' mean_cost_time= {:5f}s '.\n",
    "                         format(epoch + 1, c, binary_loss, instance_loss, train_accuracy,\n",
    "                                np.mean(train_cost_time_mean)))\n",
    "                train_cost_time_mean.clear()\n",
    "\n",
    "            if epoch % CFG.TRAIN.TEST_DISPLAY_STEP == 0:\n",
    "                log.info('Epoch_Val: {:d} total_loss= {:6f} binary_seg_loss= {:6f} '\n",
    "                         'instance_seg_loss= {:6f} accuracy= {:6f} '\n",
    "                         'mean_cost_time= {:5f}s '.\n",
    "                         format(epoch + 1, c_val, val_binary_seg_loss, val_instance_seg_loss, val_accuracy,\n",
    "                                np.mean(val_cost_time_mean)))\n",
    "                val_cost_time_mean.clear()\n",
    "\n",
    "            if epoch % 2000 == 0:\n",
    "                  # Save checkpoint, graph.pb and tensorboard\n",
    "#                 saver.save(sess, model_save_path + \"/model.ckpt\")\n",
    "#                 tf.train.write_graph(sess.graph.as_graph_def(), model_save_path + \"/\", \"graph.pb\")\n",
    "                saver.save(sess=sess, save_path=model_save_path, global_step=epoch)\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/workspace/storage/datasets/AVMSnapshots/AVM/'\n",
    "net = 'vgg'\n",
    "weights_path = '/workspace/storage/projects/lanenet-lane-detection/weights/tusimple_lanenet_vgg_2018-10-19-13-33-56.ckpt-200000'\n",
    "\n",
    "# train lanenet\n",
    "train_net(dataset_dir, weights_path, net_flag=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
